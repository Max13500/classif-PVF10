{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_pvf10 = \"../data/processed/structure_pvf_10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger le dataframe à partir du dataset\n",
    "def load_df_from_dataset(csv_path,ensemble,format):\n",
    "    # Charger le fichier CSV\n",
    "    df_pvf10 = pd.read_csv(csv_path)\n",
    "    # Filtrer par ensemble (train/test) et par format\n",
    "    df_pvf10 = df_pvf10.loc[(df_pvf10['Train_Test'] == ensemble) & (df_pvf10['Format'] == format)]   \n",
    "    return df_pvf10\n",
    "\n",
    "# Chargement dataframes d'entraînement et de test\n",
    "df_train = load_df_from_dataset(csv_pvf10, 'train','110x60')\n",
    "df_test = load_df_from_dataset(csv_pvf10, 'test','110x60')\n",
    "\n",
    "# Normalisation des chemins d'accès\n",
    "df_train['Chemin'] = df_train['Chemin'].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "df_test['Chemin'] = df_test['Chemin'].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "\n",
    "# Séparation features / cible\n",
    "X_train = df_train.drop('Classe',axis=1)\n",
    "y_train = df_train['Classe']\n",
    "X_test = df_test.drop('Classe',axis=1)\n",
    "y_test = df_test['Classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1 : vecteurs HOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformeur pour extraction des vecteurs HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Transformeur qui renvoie le dataframe des vecteurs HOG\n",
    "class HOGExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, image_size=(60,110), pixels_per_cell=(8,8),cells_per_block=(2,2)):\n",
    "        self.image_size = image_size\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    # Méthode d'extraction du vecteur HOG d'une image\n",
    "    def extract_hog(self, gray_img):\n",
    "        hog_vector = hog(\n",
    "            gray_img,\n",
    "            orientations=9,\n",
    "            pixels_per_cell=self.pixels_per_cell,\n",
    "            cells_per_block=self.cells_per_block,\n",
    "            feature_vector=True\n",
    "        )\n",
    "        return hog_vector\n",
    "    \n",
    "    # Le fit ne sert qu'à récupérer le nom des features créées\n",
    "    def fit(self, X, y=None):\n",
    "        # On utilise une seule image\n",
    "        img = cv2.imread(X['Chemin'].iloc[0])\n",
    "        # Conversion en niveaux de gris et resizing\n",
    "        img_gray_resized = cv2.cvtColor(cv2.resize(img, self.image_size), cv2.COLOR_BGR2GRAY)\n",
    "        # Extraction du vecteur HOG\n",
    "        hog_vector = self.extract_hog(img_gray_resized)\n",
    "        # On en déduit le nom des features\n",
    "        self.feature_names_ =  [f'HOG_{i+1}' for i in range(len(hog_vector))]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # On va calculer les vecteurs HOG\n",
    "        hog_vectors = []\n",
    "        # On parcourt les chemins des images\n",
    "        for img_path in X['Chemin']:\n",
    "            # Lecture de l'image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Conversion en niveaux de gris et resizing\n",
    "            img_gray_resized = cv2.cvtColor(cv2.resize(img, self.image_size), cv2.COLOR_BGR2GRAY)\n",
    "            # Extraction et stockage du vecteur HOG\n",
    "            hog_vector = self.extract_hog(img_gray_resized)\n",
    "            hog_vectors.append(hog_vector)\n",
    "        # On renvoie le dataframe des vecteurs HOG\n",
    "        return pd.DataFrame(hog_vectors,index=X.index,columns=self.feature_names_)\n",
    "    \n",
    "    # Pour récupération du nom des features créées\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "    \n",
    "# Création du transformeur\n",
    "hog_extr = HOGExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Création du StandardScaler\n",
    "hog_st = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création Pipeline HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "HOGPipeline = Pipeline(steps=[\n",
    "    (\"Extraction HOG\",hog_extr),\n",
    "    (\"Standardisation\",hog_st)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2 : vecteurs GLCM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformeur pour extraction des vecteurs GLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "# Transformeur qui renvoie le dataframe des caractéristiques GLCM\n",
    "class GLCMExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, image_size=(60,110),\n",
    "                 glcm_distances=[1],\n",
    "                 glcm_angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                 glcm_props=['contrast','dissimilarity','homogeneity','energy','correlation']):\n",
    "        self.image_size = image_size\n",
    "        self.glcm_distances = glcm_distances\n",
    "        self.glcm_angles = glcm_angles\n",
    "        self.glcm_props = glcm_props\n",
    "        # On détermine le nom des features GLCM à extraire\n",
    "        self.feature_names_ = []\n",
    "        # Pour chaque propriété, chaque distance et chaque angle\n",
    "        for prop in self.glcm_props:\n",
    "            for distance in self.glcm_distances:\n",
    "                for angle in self.glcm_angles:\n",
    "                    # On stocke dans les noms des features un label du type : contrast_d1_a45\n",
    "                    self.feature_names_.append(f\"{prop}_d{distance}_a{np.degrees(angle):.0f}\")\n",
    "\n",
    "    # Méthode d'extraction des caractéristiques GLCM d'une image\n",
    "    def extract_glcm(self, gray_img):\n",
    "        glcm_vector = []\n",
    "        # Calcul de la matrice GLCM (256 x 256 x distances x angles)\n",
    "        glcm = graycomatrix(\n",
    "            gray_img,\n",
    "            distances=self.glcm_distances,\n",
    "            angles=self.glcm_angles,\n",
    "            levels=256\n",
    "        )\n",
    "        # Pour chaque propriété GLCM\n",
    "        for prop in self.glcm_props:\n",
    "            # On la calcule pour les différentes distances et les différents angles \n",
    "            prop_matrix = graycoprops(glcm, prop)  # Matrice distances x angles\n",
    "            # On transforme la matrice en vecteur qu'on stocke dans glcm_vector\n",
    "            glcm_vector.extend(prop_matrix.flatten())\n",
    "        return glcm_vector      \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # On va calculer les vecteurs GLCM\n",
    "        glcm_vectors = []\n",
    "        # On parcourt les chemins des images\n",
    "        for img_path in X['Chemin']:\n",
    "            # Lecture de l'image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Conversion en niveaux de gris et resizing\n",
    "            img_gray_resized = cv2.cvtColor(cv2.resize(img, self.image_size), cv2.COLOR_BGR2GRAY)\n",
    "            # Extraction et stockage du vecteur GLCM\n",
    "            glcm_vector = self.extract_glcm(img_gray_resized)\n",
    "            glcm_vectors.append(glcm_vector)\n",
    "        # On renvoie le dataframe des vecteurs GLCM\n",
    "        return pd.DataFrame(glcm_vectors,index=X.index,columns=self.feature_names_)\n",
    "    \n",
    "    # Pour récupération du nom des features créées\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "    \n",
    "# Création du transformeur\n",
    "glcm_extr = GLCMExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du StandardScaler\n",
    "glcm_st = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création Pipeline GLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLCMPipeline = Pipeline(steps=[\n",
    "    (\"Extraction GLCM\",glcm_extr),\n",
    "    (\"Standardisation\",glcm_st)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3 : vecteurs Entropie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformeur pour extraction des vecteurs caractéristiques de l'entropie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Transformeur qui renvoie le dataframe des caractéristiques de l'entropie\n",
    "class EntropyExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, image_size=(60,110),radius=4, bins=10, include_histogram=True):\n",
    "        self.image_size = image_size\n",
    "        self.radius = radius\n",
    "        self.bins = bins\n",
    "        self.include_histogram = include_histogram\n",
    "        # On détermine le nom des features Entropie à extraire\n",
    "        self.feature_names_ = ['entropy_mean', 'entropy_std', 'entropy_min','entropy_max', 'entropy_median', 'entropy_skew', 'entropy_kurtosis']\n",
    "        # Si histogramme demandé\n",
    "        if include_histogram:\n",
    "            self.feature_names_.extend([f'entropy_hist_bin{i+1}' for i in range(self.bins)])\n",
    "        \n",
    "    # Méthode d'extraction des caractéristiques de l'entropie d'une image\n",
    "    def extract_entropy(self, gray_img):\n",
    "        # Calcul de la carte d'entropie\n",
    "        entropie = entropy(gray_img, disk(self.radius))\n",
    "        # Calcul des statistiques associées\n",
    "        entropy_vector = [\n",
    "                np.mean(entropie),\n",
    "                np.std(entropie),\n",
    "                np.min(entropie),\n",
    "                np.max(entropie),\n",
    "                np.median(entropie),\n",
    "                skew(entropie.ravel()),\n",
    "                kurtosis(entropie.ravel())\n",
    "            ]\n",
    "        # Si histogramme demandé\n",
    "        if (self.include_histogram):\n",
    "            hist, bin_edges = np.histogram(entropie, bins=self.bins, range=(0, np.max(entropie)), density=True)\n",
    "            # On l'ajoute au vecteur des caractéristiques de l'entropie\n",
    "            entropy_vector.extend(list(hist))\n",
    "\n",
    "        return entropy_vector      \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # On va calculer les vecteurs caractéristiques de l'entropie\n",
    "        entropy_vectors = []\n",
    "        # On parcourt les chemins des images\n",
    "        for img_path in X['Chemin']:\n",
    "            # Lecture de l'image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Conversion en niveaux de gris et resizing\n",
    "            img_gray_resized = cv2.cvtColor(cv2.resize(img, self.image_size), cv2.COLOR_BGR2GRAY)\n",
    "            # Extraction et stockage du vecteur de l'entropie\n",
    "            entropy_vector = self.extract_entropy(img_gray_resized)\n",
    "            entropy_vectors.append(entropy_vector)\n",
    "        # On renvoie le dataframe des vecteurs de l'entropie\n",
    "        return pd.DataFrame(entropy_vectors,index=X.index,columns=self.feature_names_)\n",
    "    \n",
    "    # Pour récupération du nom des features créées\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "    \n",
    "# Création du transformeur\n",
    "entropy_extr = EntropyExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du StandardScaler\n",
    "entropy_st = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création pipeline Entropie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EntropyPipeline = Pipeline(steps=[\n",
    "    (\"Entropie\",entropy_extr),\n",
    "    (\"Standardisation\",entropy_st)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4 : pixels bruts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformeur pour extraction des pixels bruts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformeur qui renvoie le dataframe des pixels bruts\n",
    "class PixelsBrutsExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, image_size=(30,55)):\n",
    "        self.image_size = image_size\n",
    "        # Nom des features : Pixel1, Pixel2...\n",
    "        self.feature_names_ = [f\"Pixel{i+1}\" for i in range(image_size[0]*image_size[1])]  \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # On va calculer les vecteurs des pixels bruts\n",
    "        brut_vectors = []\n",
    "        # On parcourt les chemins des images\n",
    "        for img_path in X['Chemin']:\n",
    "            # Lecture de l'image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Conversion en niveaux de gris et resizing\n",
    "            img_gray_resized = cv2.cvtColor(cv2.resize(img, self.image_size), cv2.COLOR_BGR2GRAY)\n",
    "            # Extraction et stockage du vecteur des pixels bruts\n",
    "            brut_vector = img_gray_resized.flatten()\n",
    "            brut_vectors.append(brut_vector)\n",
    "        # On renvoie le dataframe des vecteurs des pixels bruts\n",
    "        return pd.DataFrame(brut_vectors,index=X.index,columns=self.feature_names_)\n",
    "    \n",
    "    # Pour récupération du nom des features créées\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "    \n",
    "# Création du transformeur\n",
    "pixbrut_extr = PixelsBrutsExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du StandardScaler\n",
    "pixbrut_st = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création Pipeline Pixels Bruts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PixelsBrutsPipeline = Pipeline(steps=[\n",
    "    (\"Pixels bruts\",pixbrut_extr),\n",
    "    (\"Standardisation\",pixbrut_st)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines finales avec modèle SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Modèle SVM\n",
    "svm = SVC(kernel=\"rbf\", gamma=\"scale\", C=10)\n",
    "\n",
    "# Feature extractors et scalers déjà définis\n",
    "# -> hog_extr, hog_st, glcm_extr, glcm_st, entropy_extr, entropy_st, pixbrut_extr, pixbrut_st\n",
    "\n",
    "# Pipelines corrigés\n",
    "finalPipelinesSMOTE = {\n",
    "    \"Pixels Bruts + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Pixels bruts', pixbrut_extr),\n",
    "        ('Standardisation', pixbrut_st),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"HOG + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Extraction HOG', hog_extr),\n",
    "        ('Standardisation', hog_st),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"GLCM + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Extraction GLCM', glcm_extr),\n",
    "        ('Standardisation', glcm_st),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"Entropie + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Extraction Entropie', entropy_extr),\n",
    "        ('Standardisation', entropy_st),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"HOG/GLCM + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Preprocessing', FeatureUnion([\n",
    "            ('HOG', Pipeline([\n",
    "                ('Extraction HOG', hog_extr),\n",
    "                ('Standardisation', hog_st)\n",
    "            ])),\n",
    "            ('GLCM', Pipeline([\n",
    "                ('Extraction GLCM', glcm_extr),\n",
    "                ('Standardisation', glcm_st)\n",
    "            ]))\n",
    "        ])),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"HOG/Entropie + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Preprocessing', FeatureUnion([\n",
    "            ('HOG', Pipeline([\n",
    "                ('Extraction HOG', hog_extr),\n",
    "                ('Standardisation', hog_st)\n",
    "            ])),\n",
    "            ('Entropie', Pipeline([\n",
    "                ('Extraction Entropie', entropy_extr),\n",
    "                ('Standardisation', entropy_st)\n",
    "            ]))\n",
    "        ])),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"GLCM/Entropie + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Preprocessing', FeatureUnion([\n",
    "            ('GLCM', Pipeline([\n",
    "                ('Extraction GLCM', glcm_extr),\n",
    "                ('Standardisation', glcm_st)\n",
    "            ])),\n",
    "            ('Entropie', Pipeline([\n",
    "                ('Extraction Entropie', entropy_extr),\n",
    "                ('Standardisation', entropy_st)\n",
    "            ]))\n",
    "        ])),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "\n",
    "    \"HOG/GLCM/Entropie + SVM + SMOTE\": ImbPipeline(steps=[\n",
    "        ('Preprocessing', FeatureUnion([\n",
    "            ('HOG', Pipeline([\n",
    "                ('Extraction HOG', hog_extr),\n",
    "                ('Standardisation', hog_st)\n",
    "            ])),\n",
    "            ('GLCM', Pipeline([\n",
    "                ('Extraction GLCM', glcm_extr),\n",
    "                ('Standardisation', glcm_st)\n",
    "            ])),\n",
    "            ('Entropie', Pipeline([\n",
    "                ('Extraction Entropie', entropy_extr),\n",
    "                ('Standardisation', entropy_st)\n",
    "            ]))\n",
    "        ])),\n",
    "        ('SMOTE', SMOTE(random_state=42)),\n",
    "        ('Modeling', svm)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from copy import deepcopy\n",
    "\n",
    "def replace_sampler(pipelines, new_sampler_cls, suffix):\n",
    "    \"\"\"\n",
    "    Duplique un dictionnaire de pipelines et remplace l'étape 'SMOTE'\n",
    "    par new_sampler_cls(random_state=42). Renomme la clé pour refléter\n",
    "    le nouveau sampler.\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for name, pipe in pipelines.items():\n",
    "        steps = deepcopy(pipe.steps)           # copie profonde des étapes\n",
    "        # cherche l'étape dont le nom contient 'SMOTE'\n",
    "        for i, (step_name, step_obj) in enumerate(steps):\n",
    "            if step_name.upper() == \"SMOTE\":\n",
    "                steps[i] = (new_sampler_cls.__name__, new_sampler_cls(random_state=42))\n",
    "                break\n",
    "        new_name = name.replace(\"SMOTE\", suffix)\n",
    "        new_dict[new_name] = ImbPipeline(steps)\n",
    "    return new_dict\n",
    "\n",
    "# Dictionnaire pour SMOTE-Tomek\n",
    "finalPipelines_SMOTETomek = replace_sampler(finalPipelinesSMOTE, SMOTETomek, \"SMOTETomek\")\n",
    "\n",
    "# Dictionnaire pour ADASYN\n",
    "finalPipelines_ADASYN = replace_sampler(finalPipelinesSMOTE, ADASYN, \"ADASYN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Splits trouvés après 1 tentative(s)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    f1_score, make_scorer, accuracy_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# *Prérequis* : les objets suivants doivent déjà exister dans le notebook\n",
    "#   X_train, y_train, X_test, y_test                       (pandas ou numpy)\n",
    "#   finalPipelinesSMOTE, finalPipelines_SMOTETomek, finalPipelines_ADASYN\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) PARAMÈTRES DU CLASSIFIEUR  (modifiez ici)\n",
    "# ============================================================\n",
    "SVM_C        = 100             # rigidité (1–10 conseillé au début)\n",
    "SVM_GAMMA    = 0.01           # \"scale\", 0.1, 0.01 …\n",
    "EXTRA_PARAMS = dict(class_weight=\"balanced\")   # {} si vous ne voulez rien\n",
    "\n",
    "# Validation croisée\n",
    "N_SPLITS     = 10             # nb de folds\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Classes rares\n",
    "minority_labels = [\"break\", \"string short circuit\"]\n",
    "\n",
    "# Palette couleurs pour les graphiques\n",
    "palette = {\"SMOTE\": \"#4c72b0\", \"SMOTE-Tomek\": \"#55a868\", \"ADASYN\": \"#c44e52\"}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) SCORER MINORITAIRE\n",
    "# ============================================================\n",
    "def minority_f1_safe(y_true, y_pred, labels=minority_labels):\n",
    "    present = [lab for lab in labels if lab in y_true.values]\n",
    "    if not present:\n",
    "        return 0.0\n",
    "    return f1_score(y_true, y_pred, labels=present,\n",
    "                    average=\"macro\", zero_division=0)\n",
    "\n",
    "minority_f1 = make_scorer(minority_f1_safe, greater_is_better=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) STRATIFIED K-FOLD GARANTISSANT LES CLASSES RARES\n",
    "# ============================================================\n",
    "def safe_stratified_splits(y, labels, n_splits=N_SPLITS,\n",
    "                           max_attempts=1000, seed=RANDOM_STATE):\n",
    "    \"\"\"Retourne une liste (train_idx, val_idx) où chaque val contient ≥1 ex. de chaque label minoritaire.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    for attempt in range(max_attempts):\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=n_splits, shuffle=True,\n",
    "            random_state=rng.randint(0, 1_000_000)\n",
    "        )\n",
    "        splits = list(skf.split(np.zeros(len(y)), y))\n",
    "        if all(\n",
    "            all((y.iloc[val] == lab).sum() > 0 for lab in labels)\n",
    "            for _, val in splits\n",
    "        ):\n",
    "            print(f\"✓ Splits trouvés après {attempt+1} tentative(s)\")\n",
    "            return splits\n",
    "    raise RuntimeError(\"Impossible de satisfaire la contrainte sur les classes rares.\")\n",
    "\n",
    "safe_splits = safe_stratified_splits(y_train, minority_labels)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) BOUCLE D’ÉVALUATION\n",
    "# ============================================================\n",
    "def evaluate_family(pipelines: dict, sampler_tag: str, store: list):\n",
    "    for pipe_name, pipe in pipelines.items():\n",
    "        # -- fixe les hyper-paramètres du SVM --\n",
    "        svm_key = list(pipe.named_steps.keys())[-1]\n",
    "        pipe.named_steps[svm_key].set_params(\n",
    "            C=SVM_C, gamma=SVM_GAMMA, **EXTRA_PARAMS\n",
    "        )\n",
    "\n",
    "        # -- fit + prédiction test --\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        store.append({\n",
    "            \"Sampler\":       sampler_tag,\n",
    "            \"Pipeline\":      pipe_name,\n",
    "            \"Accuracy\":      accuracy_score(y_test, y_pred),\n",
    "            \"Macro_F1\":      f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "            \"Minority_F1\":   minority_f1_safe(y_test, y_pred),\n",
    "\n",
    "            \"CV_Minority_F1\": cross_val_score(\n",
    "                pipe, X_train, y_train,\n",
    "                scoring=minority_f1,\n",
    "                cv=safe_splits,\n",
    "                n_jobs=-1\n",
    "            ).mean()\n",
    "        })\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) EXÉCUTION SUR LES 3 FAMILLES DE PIPELINES\n",
    "# ============================================================\n",
    "results = []\n",
    "evaluate_family(finalPipelinesSMOTE,       \"SMOTE\",       results)\n",
    "evaluate_family(finalPipelines_SMOTETomek, \"SMOTE-Tomek\", results)\n",
    "evaluate_family(finalPipelines_ADASYN,     \"ADASYN\",      results)\n",
    "\n",
    "df_scores = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) VISUALISATIONS\n",
    "# ============================================================\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# (A) Minority-F1 par pipeline et sampler\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(df_scores))\n",
    "ax.bar(x, df_scores[\"Minority_F1\"],\n",
    "       color=[palette[s] for s in df_scores[\"Sampler\"]])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_scores[\"Pipeline\"], rotation=90, ha=\"center\", fontsize=8)\n",
    "ax.set_ylabel(\"Minority-F1 (test)\")\n",
    "ax.set_title(\"Impact de l’oversampling sur les classes minoritaires\")\n",
    "for lab, col in palette.items():\n",
    "    ax.bar(0, 0, color=col, label=lab)\n",
    "ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# (B) Moyenne globale par technique d’oversampling\n",
    "mean_df = (df_scores\n",
    "           .groupby(\"Sampler\")[[\"Minority_F1\", \"Macro_F1\", \"Accuracy\"]]\n",
    "           .mean()\n",
    "           .reset_index())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.bar(mean_df[\"Sampler\"], mean_df[\"Minority_F1\"],\n",
    "       color=[palette[s] for s in mean_df[\"Sampler\"]])\n",
    "ax.set_ylabel(\"Minority-F1 moyen (test)\")\n",
    "ax.set_title(\"F1 minoritaire moyen par technique d’oversampling\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Tableau récapitulatif ===\")\n",
    "display(df_scores.round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing : faire varier les paramètres des transformeurs (extracteurs + standardisation)\n",
    "- Preprocessing : intégrer rééchantillonnage (ou 'balanced'), intégrer réduction de dimension\n",
    "- Comparer l'importance des features (permutation_importance...)\n",
    "- Comparer les résultats si on retire en entrée les images 'Doute_Carre'\n",
    "- Essayer d'autres modèles : KNN, RandomForest, MLPClassifier, modèles d'ensemble\n",
    "- Question : part-on dès maintenant sur un ensemble train / val / test commun à tout le monde ?\n",
    "    - Pour chaque modèle, ajuster ses hyperparamètres sur l'ensemble train par validation croisée (GridSearch,...)\n",
    "    - Entraîner tous les modèles optimisés sur l'ensemble train et comparer leur résultat sur l'ensemble val\n",
    "    - Entraîner le meilleur modèle sur train + val, et vérifier sa bonne généralisation sur test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
